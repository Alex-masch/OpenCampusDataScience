---
title: "Datenaufbereitung f√ºr neuronales Netz"
output: html_notebook
---

###################################################
### Preparation of the Environment ####

# Clear environment
remove(list = ls())

# Create list with needed libraries
pkgs <- c("readr", "fastDummies")

# Load each listed library and check if it is installed and install if necessary
for (pkg in pkgs) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}


###################################################
### Data Import ####

# Reading the data file
data<-read_csv(file.path('Daten','umsatzdaten_kiwo_wetter_feiertage.csv',fsep = .Platform$file.sep))

###################################################
### Data Preparation ####

# Recoding of the variables into one-hot encoded (dummy) variables
dummy_list <- c("view", "condition")
daten_dummy = dummy_cols(umsatzdaten_kiwo_wetter_feiertage, dummy_list)

# Definition of lists for each one-hot encoded variable (just to make the handling easier)
condition_dummies = c('condition_1', 'condition_2', 'condition_3', 'condition_4', 'condition_5')
view_dummies = c('view_0', 'view_1', 'view_2', 'view_3','view_4')


###################################################
### Selection of the Feature Variables and the Label Variable ####

# Selection of the features (the independent variables used to predict the dependent)
#features <- c('Warengruppe' + 'Wochentag' + 'Temperatur' + 'KielerWoche' + 
    'Bewoelkung' + 'Windgeschwindigkeit', condition_dummies, view_dummies)
features <- c('Warengruppe' + 'Wochentag' + 'Temperatur' + 'KielerWoche' + 
    'Bewoelkung' + 'Windgeschwindigkeit', condition_dummies, view_dummies)
# Selection of the label (the dependent variable)
Umsatz <- 'Umsatz'


###################################################
### Selection of Training, Validation and Test Data ####

# Look at the data
str(data)

# Setting the random counter to a fixed value, so the random initialization stays the same (the random split is always the same)
set.seed(1)

# Shuffling the dataset (to get random orders within each dataset as well)
new_row_order <- sample(nrow(data))
daten_dummy <- daten_dummy[new_row_order, ]

# Assign each row number in the full dataset randomly to one of the three groups of datasets
# The probability of being in one of the groups results then in crresponding group sizes
assignment <- sample(1:3, size = nrow(daten_dummy), prob = c(.7, .2, .1), replace = TRUE)

# Create training, validation and test data for the features and the labels
training_features <- daten_dummy[assignment == 1, features]    # subset daten_dummy to training indices only
training_labels <- daten_dummy[assignment == 1, labels]    # subset daten_dummy to training indices only

validation_features <- daten_dummy[assignment == 2, features]  # subset daten_dummy to validation indices only
validation_labels <- daten_dummy[assignment == 2, labels]  # subset daten_dummy to validation indices only

test_features <- daten_dummy[assignment == 3, features]   # subset daten_dummy to test indices only
test_labels <- daten_dummy[assignment == 3, labels]   # subset daten_dummy to test indices only